import argparse
import os
import sys
from pathlib import Path

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

# Ensure the project root is in the path for module imports
sys.path.insert(0, str(Path(__file__).resolve().parent))

from main import KuulchatSpider  # Import the spider directly from main.py
from restructure_questions import restructure_json

# Define configuration
SUBJECTS_CONFIG = {
    "science": {
        "2022": "https://kuulchat.com/bece/questions/science-2022/",
        "2021": "https://kuulchat.com/bece/questions/science-2021/",
    },
    "mathematics": {
        "2022": "https://kuulchat.com/bece/questions/mathematics-2022/",
        "2021": "https://kuulchat.com/bece/questions/mathematics-2021/",
    },
}
DEFAULT_SUBJECT = "science"
DEFAULT_YEAR = "2022"


def list_available_subjects():
    """List all available subjects and years"""
    print("Available subjects and years:")
    print("=" * 40)

    for subject, years in SUBJECTS_CONFIG.items():
        print(f"\nSubject: {subject.title()}")
        print(f"Years: {', '.join(sorted(years.keys()))}")

        # Show sample URL
        sample_year = sorted(years.keys())[-1]  # Latest year
        print(f"Sample URL: {SUBJECTS_CONFIG[subject][sample_year]}")


def validate_subject_year(subject: str, year: str) -> bool:
    """Validate that subject and year combination exists"""
    if subject not in SUBJECTS_CONFIG:
        print(f"Error: Subject '{subject}' not found.")
        print(f"Available subjects: {', '.join(SUBJECTS_CONFIG.keys())}")
        return False

    if year not in SUBJECTS_CONFIG[subject]:
        print(f"Error: Year '{year}' not available for subject '{subject}'.")
        print(
            f"Available years for {subject}: {', '.join(SUBJECTS_CONFIG[subject].keys())}"
        )
        return False

    return True


def run_spider_for_subject(subject: str, year: str, base_output_dir: str = "data"):
    """
    Run the spider for a specific subject and year, then restructure and clean up.

    Args:
        subject: Subject name
        year: Year
        base_output_dir: Base directory for all output (e.g., "data")
    """
    # Validate inputs
    if not validate_subject_year(subject, year):
        return False

    # Define paths for intermediate and final files
    temp_json_input_file = Path(
        "bece_questions.json"
    )  # Temporary file for Scrapy output
    temp_csv_input_file = Path("bece_questions.csv")  # Temporary file for Scrapy output

    final_output_dir = Path(base_output_dir) / f"{subject}_{year}"

    print(f"\nStarting extraction for {subject.title()} {year}")
    print(f"Target URL: {SUBJECTS_CONFIG[subject][year]}")
    print(f"Intermediate JSON output: {temp_json_input_file}")
    print(f"Intermediate CSV output: {temp_csv_input_file}")
    print(f"Final output directory: {final_output_dir}")

    # Configure Scrapy settings
    settings = get_project_settings()
    settings.set(
        "FEEDS",
        {
            str(temp_json_input_file): {
                "format": "json",
                "overwrite": True,
                "indent": 2,
            },
            str(temp_csv_input_file): {"format": "csv", "overwrite": True},
        },
        priority="cmdline",
    )  # Use cmdline priority to ensure these settings override project settings

    settings.set("LOG_LEVEL", "INFO")
    settings.set(
        "USER_AGENT",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    )
    settings.set("DOWNLOAD_DELAY", 2)
    settings.set("RANDOMIZE_DOWNLOAD_DELAY", True)
    settings.set("CONCURRENT_REQUESTS", 1)
    settings.set("ROBOTSTXT_OBEY", True)

    process = CrawlerProcess(settings)

    # Run the spider
    start_urls = [SUBJECTS_CONFIG[subject][year]]
    process.crawl(KuulchatSpider, start_urls=start_urls)
    process.start()  # The script will block here until the crawling is finished

    print(f"\nScrapy extraction completed for {subject.title()} {year}")

    # --- Post-processing: Restructure and Cleanup ---
    print("\nStarting post-processing (restructuring and cleanup)...")
    try:
        if not temp_json_input_file.exists():
            print(
                f"Error: Intermediate JSON file '{temp_json_input_file}' not found. Skipping restructuring."
            )
            return False

        restructure_json(
            input_file=str(temp_json_input_file),
            subject=subject,
            year=year,
            output_dir=final_output_dir,
        )
        print("Post-processing completed successfully.")

        # Cleanup intermediate files generated by Scrapy
        if temp_json_input_file.exists():
            os.remove(temp_json_input_file)
            print(f"Cleaned up: {temp_json_input_file}")
        if temp_csv_input_file.exists():
            os.remove(temp_csv_input_file)
            print(f"Cleaned up: {temp_csv_input_file}")

    except Exception as e:
        print(f"Error during post-processing: {e}")
        return False

    return True


def main():
    """Main function with command-line interface"""
    parser = argparse.ArgumentParser(
        description="Run Kuulchat Educational Content Spider",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_spider.py                          # Run with defaults (science 2022)
  python run_spider.py -s mathematics -y 2021  # Run mathematics 2021
  python run_spider.py --list                  # List available subjects/years
  python run_spider.py -s science -y 2020 -o custom_data  # Custom output directory
        """,
    )

    parser.add_argument(
        "-s",
        "--subject",
        default=DEFAULT_SUBJECT,
        help=f"Subject to scrape (default: {DEFAULT_SUBJECT})",
    )

    parser.add_argument(
        "-y",
        "--year",
        default=DEFAULT_YEAR,
        help=f"Year to scrape (default: {DEFAULT_YEAR})",
    )

    parser.add_argument(
        "-o",
        "--output",
        default="data",  # Default to 'data' directory
        help="Base output directory (default: data)",
    )

    parser.add_argument(
        "--list", action="store_true", help="List available subjects and years"
    )

    args = parser.parse_args()

    # Handle list command
    if args.list:
        list_available_subjects()
        return

    # Run spider
    success = run_spider_for_subject(args.subject, args.year, args.output)

    if not success:
        sys.exit(1)

    print("\nFull process completed successfully!")


if __name__ == "__main__":
    main()
